---
title: "Final Project"
format: html
---

### Exploratory Data Analysis

First we can run some exploratory data analysis

```{r}
library(readr)
library(tidyverse)
library(janitor)
library(pheatmap)
library(corrplot)
library(ggplot2)
music_dataset <- read_csv("music_dataset.csv")
#ggplot(aes)
```

```{r, echo=FALSE}
# Cleaning up the dataset names 
tidy_music_data <- clean_names(music_dataset)

# Viewing the data
View(tidy_music_data)
```

```{r, echo=FALSE}
# Percentage + sum that each artist takes up the data
artist_sum <- tidy_music_data %>%
  group_by(artist) %>%
  summarise(n = n()) %>%
  mutate(percent = (n / 4850)*100) %>%
  arrange(desc(n))

# bar graph of the artiists + sum amount
ggplot(artist_sum, aes(x = reorder(artist, n), y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "All Artists by Count", x = "Artist", y = "Count")
```
Above in [ADD FIGURE] displays the artists list along with their track amount in the data set. We can see the top 5 artists are jay Zenith, Bass Surge, Horizon Flow, Celestial Harmony and Luna Nova. The bottom 5 artists are Samantha Lee, Aurora Soundwave ElectroPulse, Urban Rhapsody, and Retro Resonance. All artists has at leats 215 tracks and up. 
```{r}
# Top 5 Artists
top_5 <- tidy_music_data %>%
  group_by(artist) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  slice(1:5) 

# Bottom 5 
bottom_5 <- tidy_music_data %>%
  group_by(artist) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  slice(16:20) 

ranks <- top_5 %>%
  full_join(bottom_5)  %>%
  arrange(desc(n))

ggplot(ranks, aes(x = artist, y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top & Bottom 5 Artists", x = "Artist", y = "Count")
```



#Variables

#Maybe create a map of all the variables to see if any are correlated with each other then look closer at pais of varibles

```{r}
predictor_variables <-
  select(tidy_music_data, -c("song","artist", "streams", "daily_streams", "genre", "release_year"))
  
```

#heatmap

```{r}
View(predictor_variables)
pheatmap(predictor_variables, scale = "none")

```

```{r}
cor_matrix <- cor(predictor_variables)
View(cor_matrix)
```

```{r}
pheatmap(cor_matrix, scale = "none")
```

```{r}
corrplot(cor_matrix,method="pie")
```

```{r}
corrplot(cor_matrix,method="shade")

```

```{r}
high_correlations <- cor_matrix[abs(cor_matrix) > 0.5]
View(high)
```

```{r}
cor_matrix <- cor(predictor_variables, use = "complete.obs")
corrplot(cor_matrix, method = "color")
```

Our variables seem to not have any correlations with each other \# Danceability + Energy

Since these two variables might be simlar, we might want to first investigate these variables to capture any multicolinnarity. These vairables seem to be numeric and continuos,

## Part II: Describe the Data

**Data Set:** Billboard Top Songs

-   Filename: music_dataset.csv

**Date:**

-   The data set is being actively updated as of March 19, 2025.

**Summary:**

Our data was obtained from the Kaggle data set called "Billboard Top Songs" by Samay Ashar. This data set contains 5,000 song observations which were generated from both Spotify charts and artificially written musciality descriptions. The author of this data generated the observed song information to determine what factors can influence a song's weekly Billboard ranking or TikTok virality. To help in answering these questions the data includes 13 variables such as song, artist, streams, daily streams, genre, release year, peak position, weeks on chart, lyrics sentiment, tiktok virality, danceability, acousticness, and energy.

```{r}
glimpse(tidy_music_data)
```

**Further Description of Variables:**

1.  **song**: character data - the title of the song.
2.  **artist**: character data - the name of the associated artist.
3.  **streams**: discrete numerical - total number of streams a song has across all platforms.
4.  **daily_streams**: discrete numerical - the average number of streams the song receives daily.
5.  **genre**: character - the song's genre.
6.  **release_year**: discrete numerical - the year the song was released.
7.  **peak_position**: discrete numerical - highest billboard or spotify chart position.
8.  **weeks_on_chart**: discrete numerical - total weeks on the chart.
9.  **lyrics_sentiment**: continuous numerical - sentiment analysis of lyrics from -1 to +1.
10. **tik_tok_virality**: discrete numerical - popularity score of a song from 0 to 100.
11. **danceability**: discrete numerical - how danceable the song is from 0 to 1.
12. **acousticness**: discrete numerical - level of acoustics used from 0 to 1.
13. **energy**: discrete numerical - level of energy of song from 0 to 1.

Thinking abt model selections: 

We still have to think about those types of variables we want to use, and the goal of this project and 

- supervised apporach, and we can use the peak position 
- unsupervised is vetter for exploratory, is there clumping happening and this can gives us clues that something is prob not linear, how do these varibales influence herarcheal custering 
- Luclfideous distance,weve got one subgrroup of people na dhow do we characterise, is there some edvidence of a bias, and filtering

Lienar regression, 

make a new group 

discrininate anaylysis 

Look into more EDA 

pairs plot, looking at some correlation (this was done), K ean abd hearcargcia distance, shape is important

waht do those grups understand 

go a couple of different ways, are there any leading indicators, what is the smallest model I can use to get tik tok viraity 

*k means*

*heararchiam clustering *

some combiantion of number of week and what positin thay are on the chart, 

FP 1
clean the data and created a model
can include EDA and bad models


### Supervised Learning Models
# We can make it into classificition, are they in top 10 or not

```{r}
tidy_music_data <- tidy_music_data %>%
  mutate(top10 = ifelse(peak_position <= 10, 1, 0)) %>%
  mutate(top10 = as.factor(top10))  # make it a factor for classification

```












































































# Random Forest

We're taking these out because these are not really important to our model also, peak_position will be highly correlated wit
```{r}
tidy_music_data <- tidy_music_data[,!names(tidy_music_data) %in% c("song", "artist", "peak_position", "daily_streams")]
```

In this code chunk, we will use Random Forest. We are sepearating our data into a training and testing set. We will use a 80/20 split because we have a relatively big dataset with 4850 rows.

```{r}
library(randomForest)
set.seed(100)
samp <- sample(nrow(tidy_music_data), .80 * nrow(tidy_music_data))

train <- tidy_music_data[samp, ]

test <- tidy_music_data[-samp, ]

#we check the dims

dim(test)
dim(train)
```


Now we are going to make our model, this chunk means to make a random forest model predicting if the song will be in top 10 using the rest of the columns in the data set with the training data.
```{r}

model <- randomForest(top10 ~ ., data = train, ntree = 1000, mtry = 5)

model
```

In this step we make our confusion matrix this hows how well the model performed on the training set. It also elps identify how often it's correctly/incorrectly classifying songs as top 10 or not.
```{r}
model$confusion
```



Now we use our model on the testing data set, so we can test how it well it generalizes to "new" data. 
```{r}
prediction <- predict(model, newdata = test)

table(prediction, test$top10)

prediction
```

Here we see how the model did on the testing data, we see for accurate predictions over inaccurate predictions.
```{r}
results<-cbind(prediction,test$top10)

results

colnames(results)<-c('pred','real')

results<-as.data.frame(results)


# Finally, let’s calculate the accuracy of the model

sum(prediction==test$top10) / nrow(test) # The output is as shown below
```
It seems that we have aorund .91% accuracy with this model we can extract the variables that may lead to this high accuracy. We can see which variables using the importance() function as well as using the varImpPlot. The varImpPlot 

```{r}
# Get variable importance
importance(model)

varImpPlot(model)

```

We can see that in this plot different variables carry different importance. On the x-axis we have MeanDecreaseGini which shows how much each variable improves the purity of the decision trees in the forest. Higher values of MeanDecreaseGini indicates greater importance because For each variable, we track how much it helps reduce impurity on average across all the trees in the forest.The more a variable helps split the data into purer groups, the higher its MeanDecreaseGini.


```{r}
imp <- importance(model)
top_features <- rownames(imp)[order(imp[, "MeanDecreaseGini"], decreasing = TRUE)[1:5]]
top_features

```



```{r}
library(randomForest)
library(ggplot2)

imp <- importance(model)
imp_df <- data.frame(
  Variable = rownames(imp),
  MeanDecreaseGini = imp[, "MeanDecreaseGini"]
)

# Sort by importance
imp_df <- imp_df[order(imp_df$MeanDecreaseGini), ]

ggplot(imp_df, aes(x = MeanDecreaseGini, y = reorder(Variable, MeanDecreaseGini))) +
  geom_point(color = "steelblue", size = 3) +
  labs(title = "Variable Importance",
       x = "Mean Decrease Gini",
       y = "Variable") +
  scale_x_continuous(breaks = seq(0, max(imp_df$MeanDecreaseGini), by = 5)) +  # Tick every 5
  theme_minimal()

```
From the plot and from what is shown above, we can see that streams is by far the most important variable, since it has a MeanDecreaseGini of 95.Then we have lyric_sentiment being the next most important variable, since it has a MeanDecreaseGini of around 82%. Danceability and the number of weeks that the song is on the chart are the next most important variables with about 77 MeanDecreaseGini then. Then tiktok virality is next with about 76% MeanDecreaseGini.

On the other hand, variables like genre and release_year show much lower MeanDecreaseGini values, indicating that they contribute relatively little to the model’s ability to classify Top 10 songs accurately. 

The model suggests that emotional tone, how danceable a track is, how long it stays on the charts, and its popularity on TikTok all play a significant role in chart success. Overall, the plot helps us identify which features are most valuable in predicting musical success.



Next steps: Maybe look into daily streams/streams? pick one instead of other 
# Logistic Regression

# Gradient Boosting

